# -*- coding: utf-8 -*-
"""VGG_Count.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWIBxbnNwB_rweA4uuOc4sSbF7RMXejl
"""

#region  Import Libraries
from __future__ import print_function
import keras
import os
import cv2
import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input, Reshape, concatenate, PReLU
from keras.optimizers import SGD
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import confusion_matrix,classification_report
from keras.layers.normalization import BatchNormalization
from scipy.stats import norm
from keras.layers.advanced_activations import LeakyReLU

import numpy as np
import matplotlib.pyplot as plt


DATASET_PATH ='/content/Data'

IMG_HEIGHT, IMG_WIDTH  = 111, 72
CHANNELS=3
num_classes = 6
seq_steps = 2
batch_size = 10
input_shape = (IMG_HEIGHT, IMG_WIDTH, CHANNELS)

initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
print(tf.__version__)

#-------------------------------------------Data load function and generator--------------------------------------------------
DATASET_PATH ='/content/Data'

class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
   
    def __init__(self, dataset, batch_size=25, dim=input_shape, n_channels=3,
                 shuffle=True):
        'Initialization'
        # print(dataset)
        self.dim = dim
        self.dataset = dataset
        self.batch_size = batch_size
        self.seq_steps = seq_steps
        self.indexes = dataset.index
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.dataset) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.indexes[k] for k in indexes]
        # print(list_IDs_temp)
        # Generate data
        X_ordered_T1, Y_count_labels = self.__data_generation(list_IDs_temp)

        return X_ordered_T1, Y_count_labels

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.dataset))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)

        # Initialization
        X_ordered_T1 = np.empty((self.batch_size, *self.dim))
        Y_count_labels = np.empty((self.batch_size, 1), dtype=float)

        i = 0
  
        # Generate data
        for i_row, ID in enumerate(list_IDs_temp):
            # Store sample
            
            #print(self.dataset[ID])
            X_ordered_T1[i] = cv2.normalize(cv2.imread(DATASET_PATH +'/'+ self.dataset[ID][1], cv2.IMREAD_COLOR), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)  # uint8 image

            Y_count_labels[i] = self.dataset[ID][5]
            Y_count_labels = np.array(Y_count_labels).astype(int)

            i+=1

        return X_ordered_T1, Y_count_labels

def load_csv_data(data_file):
    
    if os.path.exists(DATASET_PATH +'/'+ data_file):
        with open(DATASET_PATH +'/'+ data_file, newline='') as csvfile:
            labelsfile = list(csv.reader(csvfile))
    else:
        labelsfile = [[]]

    data_file = labelsfile          # For .csv file
    
    return data_file

def load_model_data(data_file):
    
    if os.path.exists(DATASET_PATH +'/'+ data_file):
        with open(DATASET_PATH +'/'+ data_file, newline='') as csvfile:
            labelsfile = list(csv.reader(csvfile))
    else:
        labelsfile = [[]]

    data_file = labelsfile          # For .csv file

    X_ordered = np.empty((len(data_file), IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=float)
    time_diff_ordered = np.empty((len(data_file), seq_steps, 1), dtype=float)
    labels = np.empty((len(data_file), 1), dtype=float)
    images_lst = list()
    i = 0

    for i_row in data_file:
        
        if i_row[0].endswith('.jpg') and i_row[1].endswith('.jpg'):
            
            X_ordered[i] = cv2.normalize(cv2.imread(DATASET_PATH +'/'+ i_row[1], cv2.IMREAD_COLOR), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)  # uint8 image

            #X_ordered[i][1] = cv2.imread(DATASET_PATH +'/'+ i_row[2], cv2.IMREAD_COLOR)
    
            i+=1

    count_labels = [item[5] for item in data_file]
    count_labels = np.array(count_labels).astype(int)

    return X_ordered, count_labels

#-------------------------------------------Load data from CSV file--------------------------------------------------
# Set the seed value of the random number generator
X_train = load_csv_data('Train_formatted.csv')
X_val  = load_csv_data('Val_formatted.csv')
print(X_val)
print(X_train)
# Generators

training_generator = DataGenerator(dataset = X_train)
validation_generator = DataGenerator(dataset = X_val)

epochs = 800
lr_rate = 0.0015
momentum = 0.8
init = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
my_initializer = tf.keras.initializers.glorot_uniform(seed = 2)

random_seed = 0
np.random.seed(random_seed)

def create_convolution_layers(input_img):

    x = Conv2D(filters=64, kernel_size=(2, 2), activation='relu', 
                 input_shape=(input_shape))(input_img)
    x = Conv2D(filters=64, kernel_size=(2, 2), activation='relu')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.2)(x)
    x = Conv2D(filters=128, kernel_size=(2, 2), activation='relu')(x)
    x = Conv2D(filters=128, kernel_size=(2, 2), activation='relu')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.2)(x)
    x = Conv2D(filters=256, kernel_size=(2, 2), activation='relu')(x)
    x = Conv2D(filters=256, kernel_size=(2, 2), activation='relu')(x)
    x = LeakyReLU(alpha=0.2)(x)
    #x = Conv2D(filters=256, kernel_size=(1, 1), activation='relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    #x = Dropout(0.2)(x)
    #x = Conv2D(filters=512, kernel_size=(2, 2), activation='relu')(x)
    #x = Conv2D(filters=512, kernel_size=(2, 2), activation='relu')(x)
    #x = Conv2D(filters=512, kernel_size=(1, 1), activation='relu')(x)
    #x = MaxPooling2D(pool_size=(2, 2))(x)

    # x = Conv2D(filters=512, kernel_size=(2, 2), activation='relu')(x)
    # x = Conv2D(filters=512, kernel_size=(2, 2), activation='relu')(x)
    # x = MaxPooling2D(pool_size=(2, 2))(x)
    #x = Flatten()(x)
    x = Dense(500, activation='relu')(x)
    return x

frame_T1 = Input(shape=input_shape)
T1_model = create_convolution_layers(frame_T1)

x = Conv2D(filters=32, kernel_size=(2, 2), activation='relu')(T1_model)
x = Conv2D(filters=32, kernel_size=(2, 2), activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Flatten()(x)

#---------------------------------------------Total number of vehicle------------------------------------------------------------------
count_x = Dense(500, activation='softplus')(x)
count_x = Dropout(0.3)(count_x)
count_x = Dense(200, activation='softplus')(count_x)
count_x = Dropout(0.3)(count_x)
count_x = Dense(40, activation='softplus')(count_x)
count_x = Dropout(0.3)(count_x)
count_output = Dense(1, name="total_count")(count_x)
#---------------------------------------------------------------------------------------------------------------------------------------

model = Model(input=frame_T1, output=count_output)
model.summary()

# Stochastic Gradient Descent with momentum and a validation set to prevent overfitting
SGD = tf.keras.optimizers.SGD(lr=lr_rate, momentum=momentum)
adam = tf.keras.optimizers.Adam(lr=lr_rate, beta_1=0.60, beta_2=0.90)
Adadelta = tf.keras.optimizers.Adadelta(lr=lr_rate, rho=0.65)
Adamax = tf.keras.optimizers.Adamax(learning_rate=lr_rate, beta_1=0.90, beta_2=0.99, epsilon=1e-07, name="Adamax")
Adagrad = tf.keras.optimizers.Adagrad(learning_rate=lr_rate, initial_accumulator_value=0.1, epsilon=1e-07, name="Adagrad")

model.compile(loss=keras.losses.mean_squared_error,
           optimizer="Adadelta",
#           optimizer="Adamax",
#           optimizer=Adagrad,
#            optimizer=adam,
            metrics=['mae'])

earlystopper = EarlyStopping(monitor='val_loss', patience=epochs, verbose=True, mode= 'min')
checkpoint = ModelCheckpoint('./vgg_count_modelcp.ckpt', monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True, mode='auto', period=1)
#checkpoint = tf.keras.callbacks.ModelCheckpoint('.vgg_cnn_modelcp.hdf5', monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True, mode='min', period=1)

history = model.fit_generator(generator=training_generator,
          epochs = epochs,
          verbose = 1,
          callbacks=[checkpoint],
          validation_data = validation_generator)
#use data generator at fit function, can also use data generator for validation data

model.load_weights('./vgg_count_modelcp.ckpt')
#best_model = tf.keras.models.load_model('./vgg_cnn_modelcp.ckpt')

X_test_T1, Y_test_count = load_model_data('Test_formatted.csv')

Y_test_count = Y_test_count.reshape(len(Y_test_count),1)

print('X_test shape:', X_test_T1.shape)
print('Y_test shape:', Y_test_count.shape)
#--------------------------------------------------------------------------------------------------------------------

def get_prediction_interval(prediction, y_test, test_predictions, pi=.10):
    '''
    Get a prediction interval for a linear regression.
    
    INPUTS: 
        - Single prediction, 
        - y_test
        - All test set predictions,
        - Prediction interval threshold (default = .95)
    OUTPUT:
        - Prediction interval for single prediction
    '''
    
    #get standard deviation of y_test
    sum_errs = np.sum((y_test - test_predictions)**2)
    stdev = np.sqrt(1 / (len(y_test) - 2) * sum_errs)

    #get interval from standard deviation
    one_minus_pi = 1 - pi
    ppf_lookup = 1 - (one_minus_pi / 2)
    z_score = norm.ppf(ppf_lookup)
    interval = z_score * stdev

    #generate prediction interval lower and upper bound
    lower, upper = prediction - interval, prediction + interval

    print(lower, prediction, upper)

model.load_weights('./vgg_count_modelcp.ckpt')
score = model.evaluate(X_test_T1, Y_test_count, verbose=0)

print(X_test_T1.shape)

print('Test count loss:', score[0])
print('Test count MAE:', score[1])

predictions = model.predict(X_test_T1, verbose=0)

for i in range(X_test_T1.__len__()):
    # subplt = plt.subplot(int(i / 10) + 1, 10, i + 1)
    # no sense in showing labels if they don't match the letter
    #predicted_cars = np.argmax(predictions[i])
    #actual_cars = np.argmax(Y_test_count[i])
    print(predictions[i], Y_test_count[i])
    #get_prediction_interval(predictions[0][i], Y_test_new_count, predictions)

#plt.ylim(0,60)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Loss', 'val loss'], loc='upper left')
plt.show()